{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd88f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import csv\n",
    "import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74f420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file (replace with actual path if needed)\n",
    "\n",
    "# Create structure to store processed data\n",
    "\n",
    "# Helper function to extract character: <phrases> pairs from the 'summ' field\n",
    "def extract_phrases(summ):\n",
    "    char_phrases = defaultdict(list)\n",
    "    # Find all character blocks using regex pattern\n",
    "\n",
    "    pattern = r'([^:\\n]+):\\s*(.*?)(?=(?:[^:\\n]+:\\s*)|$)'\n",
    "    matches = re.findall(pattern, summ, flags=re.DOTALL)\n",
    "\n",
    "    for char, phrases in matches:\n",
    "        # split_phrases = [p.strip() for p in phrases.split(';') if p.strip()]\n",
    "        # char_phrases[char].extend(split_phrases)\n",
    "        name_clean = char.strip()\n",
    "        phrases = [p.strip() for p in phrases.replace('\\n', ' ').split(';') if p.strip()]\n",
    "        char_phrases[name_clean] = phrases\n",
    "    return char_phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ece83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best version for two characters in a row, main #####################################\n",
    "# model_name = 'llama-3.1-8B-Instruct'\n",
    "model_name = 'QWEN3-32B'\n",
    "\n",
    "save_info = \"\"\n",
    "cond = ''\n",
    "add_in = ''\n",
    "# fill in all info\n",
    "file_path = \"SUMM_COMBINE_B/\" # csv inside SUMM_COMBINE_B folder\n",
    "\n",
    "if \"two1\" in file_path:\n",
    "    save_info = \"two1\"\n",
    "\n",
    "if 'gender' in file_path:\n",
    "    cond = 'gender'\n",
    "if 'race' in file_path:\n",
    "    cond = 'race'\n",
    "if 'religions' in file_path:\n",
    "    cond = 'religions'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "combined_data = {}\n",
    "cat = \"\"\n",
    "\n",
    "processed_rows = []\n",
    "\n",
    "for location, group in df.groupby(\"location\"):\n",
    "    character_phrase_map = defaultdict(list)\n",
    "\n",
    "    # 1. Collect all phrases per character\n",
    "    for _, row in group.iterrows():\n",
    "        summ = str(row[\"summ\"])\n",
    "        id1 = str(row[\"id1\"]) if pd.notna(row[\"id1\"]) else None\n",
    "        id2 = str(row[\"id2\"]) if pd.notna(row[\"id2\"]) else None\n",
    "        bias_type = str(row[\"bias type\"])\n",
    "        id_all = row['identity'].split(',')\n",
    "\n",
    "        id1_cat = id_all[0].strip()\n",
    "        id2_cat = id_all[1].strip()\n",
    "\n",
    "        char_phrases = extract_phrases(summ)  # {id: [list of phrases]}\n",
    "\n",
    "        if id1 and id1 in char_phrases:\n",
    "            character_phrase_map[id1_cat].append(char_phrases[id1])\n",
    "        if id2 and id2 in char_phrases:\n",
    "            character_phrase_map[id2_cat].append(char_phrases[id2])\n",
    "\n",
    "    # 2. After collecting, manually construct rows\n",
    "    # Assume each character has 5 phrase lists (one per summary)\n",
    "\n",
    "    # First, determine maximum number of rows needed\n",
    "    num_rows = max(len(phrase_lists) for phrase_lists in character_phrase_map.values())\n",
    "\n",
    "    for idx in range(num_rows):\n",
    "        row_data = {\"location\": location}\n",
    "        for char_idx, (char_id, phrase_lists) in enumerate(character_phrase_map.items(), start=1):\n",
    "            row_data[f\"character_{char_idx}\"] = char_id\n",
    "            if idx < len(phrase_lists):\n",
    "                row_data[f\"phrases_{char_idx}\"] = phrase_lists[idx]\n",
    "            else:\n",
    "                row_data[f\"phrases_{char_idx}\"] = []  # Empty if not enough phrases\n",
    "        row_data[\"bias_type\"] = bias_type\n",
    "        processed_rows.append(row_data)\n",
    "\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "processed_df = pd.DataFrame(processed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43869f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = strftime('%Y%m%d-%H%M')\n",
    "concept_dir = os.path.join('SUMM_COMBINE_ALL')\n",
    "if not os.path.exists(concept_dir):\n",
    "    os.mkdir(concept_dir)\n",
    "\n",
    "save_concepts_dir = os.path.join(concept_dir, save_info + '_' + model_name + '_' + cond + '_' + add_in + '_all_summ.csv')\n",
    "processed_df.to_csv(save_concepts_dir, index = False, header=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a9971cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "# single sotry, main, #########################################################\n",
    "model_name = 'QWEN3-32B'\n",
    "\n",
    "save_info = \"\"\n",
    "cond = ''\n",
    "add_in = ''\n",
    "# fill in all info\n",
    "file_path = \"SUMM_COMBINE_B/\" # csv inside SUMM_COMBINE_B folder\n",
    "\n",
    "\n",
    "if 'gender' in file_path:\n",
    "    cond = 'gender'\n",
    "if 'race' in file_path:\n",
    "    cond = 'race'\n",
    "if 'religions' in file_path:\n",
    "    cond = 'religions'\n",
    "\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "combined_data = {}\n",
    "cat = \"\"\n",
    "\n",
    "processed_rows = []\n",
    "\n",
    "for location, group in df.groupby(\"location\"):\n",
    "    character_phrase_map = defaultdict(list)\n",
    "\n",
    "    # 1. Collect all phrases per character\n",
    "    for _, row in group.iterrows():\n",
    "        summ = str(row[\"summ\"])\n",
    "        id = str(row[\"id\"]) if pd.notna(row[\"id\"]) else None\n",
    "        bias_type = str(row[\"bias type\"])\n",
    "        id_cat = row['identity']\n",
    "\n",
    "        char_phrases = extract_phrases(summ)  # {id: [list of phrases]}\n",
    "\n",
    "        if id and id in char_phrases:\n",
    "            character_phrase_map[id_cat].append(char_phrases[id])\n",
    "\n",
    "    # 2. After collecting, manually construct rows\n",
    "    # Assume each character has 5 phrase lists (one per summary)\n",
    "\n",
    "    # First, determine maximum number of rows needed\n",
    "    num_rows = max(len(phrase_lists) for phrase_lists in character_phrase_map.values())\n",
    "\n",
    "    for idx in range(num_rows):\n",
    "        row_data = {\"location\": location}\n",
    "        for char_idx, (char_id, phrase_lists) in enumerate(character_phrase_map.items(), start=1):\n",
    "            row_data[f\"character_{char_idx}\"] = char_id\n",
    "            if idx < len(phrase_lists):\n",
    "                row_data[f\"phrases_{char_idx}\"] = phrase_lists[idx]\n",
    "            else:\n",
    "                row_data[f\"phrases_{char_idx}\"] = []  # Empty if not enough phrases\n",
    "        row_data[\"bias_type\"] = bias_type\n",
    "        processed_rows.append(row_data)\n",
    "\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "processed_df = pd.DataFrame(processed_rows)"
   ],
   "id": "aeab19b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "concept_dir = os.path.join('SUMM_COMBINE_ALL')\n",
    "if not os.path.exists(concept_dir):\n",
    "    os.mkdir(concept_dir)\n",
    "\n",
    "save_concepts_dir = os.path.join(concept_dir, \"one\" + '_' + model_name +  '_' + cond + '_all_summ.csv')\n",
    "processed_df.to_csv(save_concepts_dir, index = False, header=True)"
   ],
   "id": "06ea69b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2977763f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
